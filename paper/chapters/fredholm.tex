\documentclass[../paper.tex]{subfiles}
\begin{document}
\chapter{Альтернативный подход к задаче}
Вспомним наше изначальное интегральное уравнение:
\[
    \psi_{m,n}(x) = \int_{0}^{\infty} g(xy) f_Y(y) dy  
.\]

Преобразуем интеграл, чтобы интегрирование было по $xy$:
\[
    \psi_{m,n}(x) = \int_{0}^{\infty}  g(xy) f_Y(\frac{xy}{x}) d\frac{xy}{x}
    = \int_{0}^{\infty} \int_{0}^{\infty} g(z) \frac{1}{x} f_Y(\frac{z}{x}) dz  
.\]

Таким образом мы получили интегральное уравнение Фредгольма первого рода:
\[
    \psi_{m,n}(x) = \int_{0}^{\infty} K(x, z) g(z) dz 
.\]

Дальше мы будем использовать равномерную сетку $\left[\frac{1}{n_x}, \dots, \frac{l_x n_x}{n_x}\right]$ для $x$, $\left[\frac{1}{n_z}, \dots, \frac{l_z n_z}{n_z}\right]$ для $z$ 
и дискретизируем наше уравнение. Получаем:
\[
    \psi_{m,n}[x] = \int_{0}^{\infty} K[x,z] g[z] dz = \frac{1}{n_z} \sum_{p=1}^{l_z n_z} g\left(\frac{p}{n_z}\right) K\left[x, \frac{p}{n_z}\right]
.\]

Таким образом, мы получили систему линейных уравнений. Запишем их в матричном виде:
\[
    \bm{\psi}_{m,n} = \frac{1}{n_z} \bm{K} \bm{g}
.\]

Увеличим матрицу K, чтобы добавить регуляризацию.
\[
    \bm{\tilde K} = 
    \begin{pmatrix}
        \bm{K} \\
        \alpha \bm{E}
    \end{pmatrix}
.\]

И соответствующий $\bm{\tilde f}$:
\[
    \bm{\tilde f} = 
    \begin{pmatrix}
        \bm{f} \\
        \bm{0}
    \end{pmatrix}
.\]

И будем использовать МНК-оптимизацию. Получаем:
\[
    \bm{g_*} = \argmin_{\bm{g}} \| \bm{\tilde K} \bm{g} - \bm{f} \|
.\]
%
\section{Градиентный спуск}
Вместо процедур для решения МНК-задачи мы можем использовать метод градиентного
спуска. Будем использовать матричное представление
\[
  \bm{\psi}_{m,n} = \frac{1}{n_z} \bm{K} \bm{g}
.\]

Тогда можно ввести функцию потери $L(\bm{\psi_{m,n}}, \bm{\hat{\psi}_{m,n}})$,
где $\bm{\hat{\psi}_{m,n}} = \bm{K} \bm{\hat{g}_{m,n}}$, а $\bm{\hat{g}_{m,n}}$ ---
оценка для $\bm{g_{m,n}}$.

В частности, будем рассматривать следующие функции потерь:
\begin{itemize}
\item $l1$-потеря: $L(\bm{x}, \bm{y}) = \| x - y \|_1$;
\item $l2$-потеря: $L(\bm{x}, \bm{y}) = \| x - y \|_2$;
\item функция потери Хьюбера:
  \[
    L(x, y) =
    \begin{cases}
      \frac{1}{2} (x-y)^2 \text{, при $|x-y| \leqslant 1$} \\
      |x-y| - \frac{1}{2} \text{, при $|x-y| > 1$}
    \end{cases}.
  \]
  \[
    L(\bm{x}, \bm{y}) = \frac{1}{n} \sum_{i=1}^k L(x_i, y_i)
  \]
\end{itemize}

Для каждой из них будем использовать $L_1$- или $L_2$-регуляризацию:
\[
  \tilde{L}(\bm{\psi_{m,n}}, \bm{\hat{\psi}_{m,n}}) = L(\bm{\psi_{m,n}}, \bm{\hat{\psi}_{m,n}}) + \| \bm{g_{m,n}} - \bm{\hat{g}_{m,n}} \|
\]

\section{Итеративные методы}
В статье \cite{fredholm-integrals} рассматриваются итеративные методы решения задачи Фредгольма
первого рода: аддитивный и мультипликативный.

В приложении к задаче аддитивный метод использует следующие итерации:
\[
  g_{m,n;k}(z) = g_{m,n;k-1}(z) = \int_0^\infty K(x, z) (\psi_{m,n;k}(x) - \psi_{m,n}(x)) dx
,\]
\[
  \psi_{m,n;k}(x) = \int_0^\infty K(x, z) g_{m,n;k}(z) dz
.\]

Для мультипликативного метода используются такие итерации:
\[
  g_{m,n;k}(z) = \frac{g_{m,n;k-1}(z)}{\int_{0}^\infty K(x, z) dx} \int_{0}^\infty \frac{K(x, z) \psi_{m,n}(x)}{\psi_{m,n;k}(x)} dx
,\]
\[
  \psi_{m,n;k}(x) = \int_0^\infty K(x, z) g_{m,n;k}(z) dz
.\]

Так как $\psi$ и $g$ могут принимать отрицательные значения, производится
следующее преобразование: выбирается параметр $t$, $\psi_{m,n}$ заменяется
на $\tilde{\psi}_{m,n} = \psi_{m,n} + t$, $f_{m,n;0}$ заменятся на
$\tilde{f}_{m,n;0} = f_{m,n;0} + t$.
%
\section{Поправка для оценок}
Будем также использовать поправку, предложенную в статье \cite{correction-of-density-estimation}
В ней рассматриваются два случая: когда интеграл
\[
  \int \max(\hat{f}(x), 0) dx
\]
больше 1, и когда меньше единицы.

В первом случае оценка $\hat{f}$ заменяется на $\tilde{f}(x) = \max(0, \hat{f}(x) - \xi)$,
где $\xi$ выбирается так, чтобы выполнялось
\[
  \int \tilde{f}(x) dx = 1
.\]

Во втором случае используется оценка
\[
  \tilde{f}(x) = \tilde{f}(x; M) =
  \begin{cases}
    \max(0, \hat{f}(x)) + \eta_M \text{, для $|x| \leqslant M$,} \\
    \max(0, \hat{f}(x)) \text{, для $|x| > M$,}
  \end{cases}
\]
где
\[
  \eta_M = \frac{1}{2M} \left( 1 - \int \max(0, \hat{f}(x)) dx \right)
.\]
\end{document}
